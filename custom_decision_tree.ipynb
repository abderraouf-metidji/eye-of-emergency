{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons dans la colonne 'text' du DataFrame : 110\n"
     ]
    }
   ],
   "source": [
    "nombre_doublons = df['text'].duplicated().sum()\n",
    "print(f\"Doublons dans la colonne 'text' du DataFrame : {nombre_doublons}\")\n",
    "\n",
    "df = df.drop_duplicates(subset='text', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   target  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons dans la colonne 'text' du DataFrame : 20\n"
     ]
    }
   ],
   "source": [
    "nombre_doublons = df_test['text'].duplicated().sum()\n",
    "print(f\"Doublons dans la colonne 'text' du DataFrame : {nombre_doublons}\")\n",
    "\n",
    "df_test = df_test.drop_duplicates(subset='text', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation de la pipeline de donnée avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction des fonctions / Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "custom_stopwords = set([\"oh\", \"please\", \"help\", \"#\", \"@\"])\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9#@\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def spacy_pipeline(text):\n",
    "    hashtags_mentions = re.findall(r'[#@]\\w+', text)\n",
    "    text = re.sub(r'[#@]\\w+', '', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.text not in custom_stopwords and token.is_alpha]\n",
    "\n",
    "    # Mettre les hashtags et mentions à la fin du traitement\n",
    "    tokens.extend(hashtags_mentions)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Pipeline de traitement de texte\n",
    "def text_pipeline(text):\n",
    "    text = clean_text(text)\n",
    "    return spacy_pipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing pipeline\n",
    "df['processed_text'] = df['text'].apply(text_pipeline)\n",
    "df_test['processed_text'] = df_test['text'].apply(text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['processed_text']).toarray()\n",
    "X_test = vectorizer.transform(df_test['processed_text']).toarray()\n",
    "\n",
    "y = df['target'].values  # Ensure y is a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7503, 5000)\n",
      "(7503,)\n"
     ]
    }
   ],
   "source": [
    "# shapes\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, prediction=None):\n",
    "        self.feature = feature        # Index of feature to split on\n",
    "        self.threshold = threshold    # Threshold value for feature\n",
    "        self.prediction = prediction  # Prediction value at leaf node\n",
    "        self.left = None              # Left child Node\n",
    "        self.right = None             # Right child Node\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs, self.root) for inputs in X]\n",
    "\n",
    "    def _predict(self, inputs, node):\n",
    "        while node.left and node.right:\n",
    "            if inputs[node.feature] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1 or n_samples < self.min_samples_split:\n",
    "            return Node(prediction=self._most_common_label(y))\n",
    "\n",
    "        # Find best split\n",
    "        best_gini = 1.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                y_left = y[X[:, feature] < threshold]\n",
    "                y_right = y[X[:, feature] >= threshold]\n",
    "                gini = (len(y_left) * self._gini_impurity(y_left) + len(y_right) * self._gini_impurity(y_right)) / n_samples\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        # Create split\n",
    "        if best_gini < 1.0:\n",
    "            left_idx = X[:, best_feature] < best_threshold\n",
    "            X_left, y_left = X[left_idx], y[left_idx]\n",
    "            X_right, y_right = X[~left_idx], y[~left_idx]\n",
    "            node = Node(feature=best_feature, threshold=best_threshold)\n",
    "            node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "            node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "            return node\n",
    "\n",
    "        # Leaf node (no split performed)\n",
    "        return Node(prediction=self._most_common_label(y))\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        class_counts = np.bincount(y)\n",
    "        class_probs = class_counts / len(y)\n",
    "        return 1.0 - np.sum(class_probs ** 2)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        if len(counter) == 0:\n",
    "            return None  # Handle the case where Counter is empty\n",
    "        return counter.most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree - Evaluation Set\n",
      "Accuracy: 0.6575616255829447\n",
      "Precision: 0.8192090395480226\n",
      "Recall: 0.23125996810207336\n",
      "F1 Score: 0.36069651741293535\n",
      "Custom Decision Tree - Test Set\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Split the data for training and evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Custom Decision Tree\n",
    "custom_tree = CustomDecisionTree(max_depth=10)\n",
    "custom_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "y_eval_pred = custom_tree.predict(X_eval)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('Custom Decision Tree - Evaluation Set')\n",
    "print('Accuracy:', accuracy_score(y_eval, y_eval_pred))\n",
    "print('Precision:', precision_score(y_eval, y_eval_pred))\n",
    "print('Recall:', recall_score(y_eval, y_eval_pred))\n",
    "print('F1 Score:', f1_score(y_eval, y_eval_pred))\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = custom_tree.predict(X_test)\n",
    "\n",
    "# Test scores\n",
    "print('Custom Decision Tree - Test Set')\n",
    "print('Accuracy:', accuracy_score(df_test['target'], y_test_pred))\n",
    "print('Precision:', precision_score(df_test['target'], y_test_pred))\n",
    "print('Recall:', recall_score(df_test['target'], y_test_pred))\n",
    "print('F1 Score:', f1_score(df_test['target'], y_test_pred))\n",
    "\n",
    "\n",
    "# Save predictions to the test dataframe\n",
    "df_test['target'] = y_test_pred\n",
    "df_test.to_csv('test_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergency-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
