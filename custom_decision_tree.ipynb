{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe le csv d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche les 5 premières lignes pour visualiser le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons dans la colonne 'text' du DataFrame : 110\n"
     ]
    }
   ],
   "source": [
    "nombre_doublons = df['text'].duplicated().sum()\n",
    "print(f\"Doublons dans la colonne 'text' du DataFrame : {nombre_doublons}\")\n",
    "\n",
    "df = df.drop_duplicates(subset='text', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les doublons du dataset en les cherchant dans la colonne *text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe le dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   target  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche encore une fois les 5 premières lignes afin de le visualiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons dans la colonne 'text' du DataFrame : 20\n"
     ]
    }
   ],
   "source": [
    "nombre_doublons = df_test['text'].duplicated().sum()\n",
    "print(f\"Doublons dans la colonne 'text' du DataFrame : {nombre_doublons}\")\n",
    "\n",
    "df_test = df_test.drop_duplicates(subset='text', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime également les doublons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation de la pipeline de donnée avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction des fonctions / Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "custom_stopwords = set([\"oh\", \"please\", \"help\", \"#\", \"@\"])\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9#@\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def spacy_pipeline(text):\n",
    "    hashtags_mentions = re.findall(r'[#@]\\w+', text)\n",
    "    text = re.sub(r'[#@]\\w+', '', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.text not in custom_stopwords and token.is_alpha]\n",
    "\n",
    "    # Mettre les hashtags et mentions à la fin du traitement\n",
    "    tokens.extend(hashtags_mentions)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Pipeline de traitement de texte\n",
    "def text_pipeline(text):\n",
    "    text = clean_text(text)\n",
    "    return spacy_pipeline(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on construit notre pipeline avec quelques mots custom qui ont été identifiés dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the optimized spacy_pipeline to the 'text' column\n",
    "df['processed_text'] = df['text'].apply(spacy_pipeline)\n",
    "df_test['processed_text'] = df_test['text'].apply(spacy_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on applique la pipeline sur notre colonne *text* de nos deux dataset en créeant une nouvelle colonne nommée *processed_text* qui contient les modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['processed_text']).toarray()\n",
    "X_test = vectorizer.transform(df_test['processed_text']).toarray()\n",
    "\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vectorize la nouvelle colonne des deux dataset afin d'appliquer l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, prediction=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.prediction = prediction\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs, self.root) for inputs in X]\n",
    "\n",
    "    def _predict(self, inputs, node):\n",
    "        while node.left and node.right:\n",
    "            if inputs[node.feature] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1 or n_samples < self.min_samples_split:\n",
    "            return Node(prediction=self._most_common_label(y))\n",
    "\n",
    "        # Find best split\n",
    "        best_gini = 1.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                y_left = y[X[:, feature] < threshold]\n",
    "                y_right = y[X[:, feature] >= threshold]\n",
    "                gini = (len(y_left) * self._gini_impurity(y_left) + len(y_right) * self._gini_impurity(y_right)) / n_samples\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        # Create split\n",
    "        if best_gini < 1.0:\n",
    "            left_idx = X[:, best_feature] < best_threshold\n",
    "            X_left, y_left = X[left_idx], y[left_idx]\n",
    "            X_right, y_right = X[~left_idx], y[~left_idx]\n",
    "            node = Node(feature=best_feature, threshold=best_threshold)\n",
    "            node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "            node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "            return node\n",
    "\n",
    "        return Node(prediction=self._most_common_label(y))\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        class_counts = np.bincount(y)\n",
    "        class_probs = class_counts / len(y)\n",
    "        return 1.0 - np.sum(class_probs ** 2)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        if len(counter) == 0:\n",
    "            return None\n",
    "        return counter.most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de notre classe custom de decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree - Evaluation Set\n",
      "Accuracy: 0.6282478347768155\n",
      "Precision: 0.847457627118644\n",
      "Recall: 0.15625\n",
      "F1 Score: 0.2638522427440633\n"
     ]
    }
   ],
   "source": [
    "# Split the data for training and evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Train the Custom Decision Tree\n",
    "custom_tree = CustomDecisionTree(max_depth=5)\n",
    "custom_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "y_eval_pred = custom_tree.predict(X_eval)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('Custom Decision Tree - Evaluation Set')\n",
    "print('Accuracy:', accuracy_score(y_eval, y_eval_pred))\n",
    "print('Precision:', precision_score(y_eval, y_eval_pred))\n",
    "print('Recall:', recall_score(y_eval, y_eval_pred))\n",
    "print('F1 Score:', f1_score(y_eval, y_eval_pred))\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = custom_tree.predict(X_test)\n",
    "\n",
    "# Save predictions to the test dataframe\n",
    "df_test['target'] = y_test_pred\n",
    "df_test.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir grace à ces métriques que notre algorithme custom a de bons score au niveau de l'accuracy et de la precision mais que le recall est assez bas à 0.19. Cela nous donne un score F1 global de 0.30 ce qui n'est pas un bon score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Decision Tree - Evaluation Set\n",
      "Accuracy: 0.6289140572951366\n",
      "Precision: 0.8487394957983193\n",
      "Recall: 0.1578125\n",
      "F1 Score: 0.26613965744400525\n"
     ]
    }
   ],
   "source": [
    "# Train the Decision Tree\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=5)\n",
    "sklearn_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "y_eval_pred = sklearn_tree.predict(X_eval)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('Sklearn Decision Tree - Evaluation Set')\n",
    "print('Accuracy:', accuracy_score(y_eval, y_eval_pred))\n",
    "print('Precision:', precision_score(y_eval, y_eval_pred))\n",
    "print('Recall:', recall_score(y_eval, y_eval_pred))\n",
    "print('F1 Score:', f1_score(y_eval, y_eval_pred))\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = sklearn_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe SKLearn nous donne des résultats similaires avec des paramètres identiques. On peut voir cependant que le recall est plus bas ce qui fait que notre score F1 est plus bas aussi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergency-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
